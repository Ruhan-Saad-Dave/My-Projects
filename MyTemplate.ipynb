{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxOSC17in59APr+HiPaz/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruhan-Saad-Dave/My-Projects/blob/main/MyTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "xexaOaKoSSs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary libraries"
      ],
      "metadata": {
        "id": "ANW1rruQi6tU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqVT4gi1i3uH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScalar\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing dataset"
      ],
      "metadata": {
        "id": "FOmlTy8ojcEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = pd.read_csv('Data.csv')\n",
        "df_excel = pd.read_excel('Data.xlsx')\n",
        "df_json = pd.read_json('Data.json')\n",
        "df_txt = pd.read_csv('Data.txt', sep = '\\t')\n",
        "df_hdf = pd.read_hdf('Data.h5', key = 'data')    #HDF% (Hierarchical Data Format version 5)\n",
        "#No method for XML (eXtensible Markup Language) file\n",
        "df_parquet = pd.read_parquet('Data.parquet')\n",
        "#SQLite Database\n",
        "'''\n",
        "import sqlite3\n",
        "conn = sqlite3.connect('database.db)\n",
        "'''\n",
        "df_sql = pd.read_sql_query('SELECT * FROM table_name', conn)\n",
        "#No method for big query"
      ],
      "metadata": {
        "id": "BHQVz-6OjebV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Information"
      ],
      "metadata": {
        "id": "flrzkRe1lNXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.tail()\n",
        "df.info()\n",
        "df.describe()\n",
        "df.describe(include = 'object')\n",
        "print(df.shape)    #Rows and Columns count\n",
        "print(df.columns)\n",
        "print(df.index)\n",
        "print(df.dtypes)\n",
        "df['column_name'].value_counts()    #Count unique values in column\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "tTaID7oqlPyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Check for Null value"
      ],
      "metadata": {
        "id": "H-5uD2Flmibe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull()\n",
        "df.notnull()\n",
        "df.isna()\n",
        "df.notna()\n",
        "df.isnull().sum()    #Sum of null values in each column\n",
        "df.isnull().any()    #Check if any null values exist\n",
        "df.isnull().all()    #Check if all are null\n",
        "df.isnull().sum().value_counts()    #Count number of null values in each column"
      ],
      "metadata": {
        "id": "jNf8TaJ7mlbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Replace Null values"
      ],
      "metadata": {
        "id": "5ToF7w4Pnw90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(value)    #Scalar, dictionary, Series or DataFrame\n",
        "df.replace(to_replace = np.nan, value)    #Replace specific values\n",
        "df.interpolate(method = 'linear')    #method = (linear, quadratic, nearest)\n",
        "df.bfill()    #Backward fill: replace null with next non null\n",
        "df.ffill()    #Forward fill: replace null with previous non null\n",
        "df.dropna(axis = 0)    #Drop rows with null\n",
        "df.dropna(axis = 1)    #Drop columns with null\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'mean')    #also for median, mode or constant\n",
        "df = imputer.fit_transform(df)\n",
        "#or\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)\n",
        "\n",
        "df['column_name'].fillna(df['column_name'].mean())    #or .median or .mode()[0]\n",
        "\n",
        "def custom_replace(value):\n",
        "  if pd.isnull(value):\n",
        "    return custom_value\n",
        "  else:\n",
        "    return value\n",
        "df['column_name'].apply(custom_replace)"
      ],
      "metadata": {
        "id": "fYsi0Oznnztd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graph representation"
      ],
      "metadata": {
        "id": "eRsctfzbqOLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#matplotlib\n",
        "plt.hist(data, bins = 10)    #Histogram\n",
        "plt.bar(categories, values)    #Bar plot\n",
        "plt.plot(x, y)    #Line plot\n",
        "plt.scatter(x,y)    #Scatter plot\n",
        "plt.boxplot(data)    #Box plot\n",
        "plt.pie(values, labels = categories)    #Pie charts\n",
        "plt.fill_between(x,y1,y2)    #Area plot\n",
        "\n",
        "#seaborn\n",
        "sns.heatmap(data)    #Heatmaps\n",
        "sns.pairplot(df)    #Pair plot\n",
        "sns.violinplot(x = 'category', y = 'value', data = df)    #Combining box plots and kernel density plot\n",
        "sns.jointplot(x = 'x', y = 'y', data = df)\n",
        "\n",
        "#example\n",
        "plt.figure(figsize = (8,6))\n",
        "sns.histplot(df_imputed['numerical_columns'], bins = 20, kde = True, color = 'skyblue')\n",
        "plt.title('Title')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLu-ueI0qQ3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoding Labels"
      ],
      "metadata": {
        "id": "b0s7QeDwrf5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ordinal Encoding\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "encoder = OrdinalEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "#ON Hot Encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "#Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "#Frequency Encoding\n",
        "frequency_map = data['column_name'].value_counts().to_dict()\n",
        "data['column_name'] = data['column_name'].map(frequency_map)\n",
        "\n",
        "#Target Encoding\n",
        "mean_map = data.groupby('column_name')['target_variable'].mean().to_dict()\n",
        "data['column_name'] = data['column_name'].map(mean_map)\n",
        "\n",
        "#Binary Encoding\n",
        "import category_encoders as ce\n",
        "encoder = ce.BinaryEncoder(cols=['column_name'])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "#Hashing Encoding\n",
        "import category_encoders as ce\n",
        "encoder = ce.HashingEncoder(cols=['column_name'])\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "\n",
        "#Dummy Encoding\n",
        "pd.get_dummies(data, drop_first=True)"
      ],
      "metadata": {
        "id": "EtLaXB3_sEg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting Training and Test Set"
      ],
      "metadata": {
        "id": "nUpifh_ANuou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Holdout method\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#K-Fold Cross-Validation\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=k)\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#Stratified K-Fold Cross-Validation\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=k)\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#Leave-One-Out (LOO) Cross-Validation\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "loo = LeaveOneOut()\n",
        "for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#Time Series Split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=k)\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#Custom Splitting\n",
        "# Example of custom splitting logic\n",
        "train_indices = ...\n",
        "test_indices = ...\n",
        "X_train, X_test = X[train_indices], X[test_indices]\n",
        "y_train, y_test = y[train_indices], y[test_indices]"
      ],
      "metadata": {
        "id": "Y2_7ZAVqN3KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Scaling"
      ],
      "metadata": {
        "id": "VnAfvFwxOVXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizatin (Z-score normalization)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Min-Max Scaling(Normalization)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Robust Scaling\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Max Abs Scaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Power Transformation\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "scaler = PowerTransformer(method='yeo-johnson')\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Quantile Transformation\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "scaler = QuantileTransformer(output_distribution='uniform')\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Unit Vector Scaling\n",
        "from sklearn.preprocessing import Normalizer\n",
        "scaler = Normalizer()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "#Log Transformation\n",
        "import numpy as np\n",
        "scaled_data = np.log1p(data)"
      ],
      "metadata": {
        "id": "H5qZDcRrOYsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notes"
      ],
      "metadata": {
        "id": "ppvycoBDSZeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Numpy"
      ],
      "metadata": {
        "id": "wbRAI4cKSffo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLLgS6KPShaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas"
      ],
      "metadata": {
        "id": "TnemIz8fShuy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5F56VzCqSjOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matplotlib"
      ],
      "metadata": {
        "id": "V4yS5kheSjt5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kCW911beSlce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seaborn"
      ],
      "metadata": {
        "id": "J7eXA_onSlxT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4mhm6GeSoYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scikit-learn"
      ],
      "metadata": {
        "id": "wOmzPjFCSo37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn."
      ],
      "metadata": {
        "id": "LZ7dKvuqSrJY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}